{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ead67d1-f530-401d-ad52-5448611fabb1",
   "metadata": {},
   "source": [
    "# Generate Datasets for Figure 2 (Co)MIGHT\n",
    "\n",
    "Here, we would like to generate the following datasets that contain the tuple (X1, X2, y), where X1, X2 and y have different structural and functional relationships. The structural relationships are there to help determine what relationships each variable set has with respect to another. The functional relationships determine what sort of functional dependencies each has on another. One can interpret these graphical model relationships \"causally\", but for our purposes, we simply utilize their probabilistic constraints. \n",
    "\n",
    "In this figure, we will specifically address the questions:\n",
    "\n",
    "- Can we detect when X2 has additional information about y conditioned on X1?\n",
    "- Can we capture the signal X2 provides about y conditioned on X1?\n",
    "\n",
    "1. (X1 -> y <- X2): This demonstrates that X1 and X2 both provide information on y, but are themselves independent. The arrowhead edge can have different functional relationships. In this case, we will use X1 -> y as \"linear\" and \"X2 -> y\" as \"log\", meaning there is a linear relationship. \n",
    "\n",
    "2. (X1 -> y <- X2; X1 -> X2): This means X1 and X2 both provide information on y, but the information provided by X1 is both direct and mediated by X2. Here, we will use X1 and X2 relationship with y as \"linear\", but X2 is a noisy nonlinear transformation of X1. X2 is conditionally dependent of y given X1.\n",
    "\n",
    "3. (X2 <- X1 -> y): Each relationship is linear. In this case, X2 is conditionally independent of y given X1.\n",
    "\n",
    "4. (X1 -> y; X2): Each relationship is linear. In this case, again X2 is conditionally independent of y given X1.\n",
    "\n",
    "Here, we present different ways that X1, X2 and y are structurally related, such that the null hypothesis of `X2 \\perp y | X1` is tested. We also change the relationship of X2 in various settings. However, the important part is generating data with different conditional independence relationships with respect to y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2088a7db-a98a-40bf-9a23-c9d3c12a7d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "28cfbe01-eea7-4f07-9ff6-37a09ae832a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from hyppo.conditional import ConditionalDcorr\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.special import expit\n",
    "from scipy.stats import ortho_group\n",
    "from sklearn.datasets import (\n",
    "    make_blobs,\n",
    "    make_classification,\n",
    "    make_sparse_spd_matrix,\n",
    "    make_spd_matrix,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sktree import HonestForestClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sktree.datasets.multiview import make_gaussian_mixture, make_joint_factor_model\n",
    "from sktree.stats import (\n",
    "    FeatureImportanceForestClassifier,\n",
    "    FeatureImportanceForestRegressor,\n",
    "    PermutationForestRegressor,\n",
    "    PermutationTest,\n",
    ")\n",
    "from sktree.stats.utils import (\n",
    "    METRIC_FUNCTIONS,\n",
    "    POSITIVE_METRICS,\n",
    "    POSTERIOR_FUNCTIONS,\n",
    "    REGRESSOR_METRICS,\n",
    "    _compute_null_distribution_coleman,\n",
    "    _non_nan_samples,\n",
    ")\n",
    "from sktree.tree import DecisionTreeClassifier, MultiViewDecisionTreeClassifier\n",
    "\n",
    "seed = 12345\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e80939b6-9ed2-4c08-af72-5ff92205158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(n, p, noise=False, coeffs=None):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    eps = np.random.normal(size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.0022 * i) for i in range(p)])\n",
    "    y = x * coeffs + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def exponential(n, p, noise=False, coeffs=None):\n",
    "    x = np.random.normal(scale=3, size=(n, p))\n",
    "    eps = np.random.normal(scale=3, size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.022 * (i + 52)) for i in range(p)])\n",
    "    y = np.exp(x * coeffs) - 1 + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def cubic(n, p, noise=False, coeffs=None):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    eps = np.random.normal(size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.031 * (i + 25)) for i in range(p)])\n",
    "\n",
    "    x_coeffs = x * coeffs\n",
    "    y = x_coeffs**3 + x_coeffs**2 + x_coeffs + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def step(n, p, noise=False, coeffs=None):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.0457 * (i + 10)) for i in range(p)])\n",
    "    eps = np.random.normal(size=(n, p))\n",
    "\n",
    "    x_coeff = ((x * coeffs) > 0.5) * 1\n",
    "    y = x_coeff + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def quadratic(n, p, noise=False, coeffs=None):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.0325 * (i + 24)) for i in range(p)])\n",
    "    eps = np.random.normal(size=(n, p))\n",
    "\n",
    "    x_coeffs = x * coeffs\n",
    "    y = x_coeffs**2 + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def w_shaped(n, p, noise=False, coeffs=None):\n",
    "    x = np.random.normal(scale=30, size=(n, p))\n",
    "    u = np.random.normal(scale=30, size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.2735 * (i + 10)) for i in range(p)])\n",
    "    eps = np.random.normal(scale=30, size=(n, p))\n",
    "    x_coeffs = x * coeffs\n",
    "    u_coeffs = u * coeffs\n",
    "    y = ((x_coeffs**4) - 7 * x_coeffs**2) + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def logarithmic(n, p, noise=False, coeffs=None):\n",
    "    rng = np.random.default_rng()\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.0072 * i) for i in range(p)])\n",
    "\n",
    "    sig = np.identity(p)\n",
    "    x = rng.standard_normal(size=(n, p))\n",
    "    eps = rng.standard_normal(size=(n, p))\n",
    "\n",
    "    y = np.log((x * coeffs + 1) ** 2) + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def fourth_root(n, p, noise=False, coeffs=None):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    eps = np.random.normal(size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.25 * (i + 50)) for i in range(p)])\n",
    "\n",
    "    x_coeffs = x * coeffs\n",
    "    y = 10 * np.abs(x_coeffs) ** 0.25 + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def _sin(n, p, noise=False, period=4 * np.pi, coeffs=None):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    if period == 4 * np.pi and coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.0095 * (i + 50)) for i in range(p)])\n",
    "    elif period == 16 * np.pi and coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.015 * (i + 50)) for i in range(p)])\n",
    "    x = rng.normal(size=(n, p))\n",
    "    sig = np.identity(p)\n",
    "    v = rng.multivariate_normal(np.zeros(p), sig, size=n, method=\"cholesky\")\n",
    "    eps = rng.normal(size=(n, p))\n",
    "\n",
    "    y = np.sin(x * coeffs * period) + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def sin_four_pi(n, p, noise=False, coeffs=None):\n",
    "    return _sin(n, p, noise=noise, period=4 * np.pi, coeffs=coeffs)\n",
    "\n",
    "\n",
    "def sin_sixteen_pi(n, p, noise=False, coeffs=None):\n",
    "    return _sin(n, p, noise=noise, period=16 * np.pi, coeffs=coeffs)\n",
    "\n",
    "\n",
    "def _square_diamond(n, p, noise=False, low=-1, high=1, period=-np.pi / 2, coeffs=None):\n",
    "    u = np.random.uniform(low, high, size=(n, p))\n",
    "    v = np.random.uniform(low, high, size=(n, p))\n",
    "    sig = np.identity(p)\n",
    "    eps = np.random.uniform(low, high, size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.0042 * (i + 10)) for i in range(p)])\n",
    "\n",
    "    x = u * np.cos(period) + v * np.sin(period)\n",
    "    y = -u * coeffs * np.sin(period) + v * coeffs * np.cos(period) + eps * noise\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def square(n, p, noise=False, low=-1, high=1, coeffs=None):\n",
    "    return _square_diamond(\n",
    "        n, p, noise=noise, low=low, high=high, period=-np.pi / 8, coeffs=coeffs\n",
    "    )\n",
    "\n",
    "\n",
    "def two_parabolas(n, p, noise=False, prob=0.5, coeffs=None):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    if coeffs is None:\n",
    "        coeffs = np.array([np.exp(-0.00145 * (i + 50)) for i in range(p)])\n",
    "    u = np.random.binomial(1, prob, size=(n, 1))\n",
    "    eps = np.random.normal(size=(n, p))\n",
    "\n",
    "    x_coeffs = x * coeffs\n",
    "    y = (x_coeffs**2) * (u - 0.5) + noise * eps\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def diamond(n, p, noise=False, low=-1, high=1, coeffs=None):\n",
    "    return _square_diamond(\n",
    "        n, p, noise=noise, low=low, high=high, period=-np.pi / 4, coeffs=coeffs\n",
    "    )\n",
    "\n",
    "\n",
    "def multimodal_independence(n, p, prob=0.5, sep1=3, sep2=2):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    sig = np.identity(p)\n",
    "    u = rng.multivariate_normal(np.zeros(p), sig, size=n, method=\"cholesky\")\n",
    "    v = rng.multivariate_normal(np.zeros(p), sig, size=n, method=\"cholesky\")\n",
    "    u_2 = rng.binomial(1, prob, size=(n, p))\n",
    "    v_2 = rng.binomial(1, prob, size=(n, p))\n",
    "\n",
    "    x = u / sep1 + sep2 * u_2 - 1\n",
    "    y = v / sep1 + sep2 * v_2 - 1\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "345c3acd-a58b-480c-bf04-8efd006fe0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_log_collider(\n",
    "    n_samples,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    seed,\n",
    "    noise_1=False,\n",
    "    noise_2=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    This generates a X1 -> y <- X2 where X2's log is correlated with y.\n",
    "\n",
    "    X1 ~ N(0, 1) for n_features. Then X1 is concatenated with noise dimensions N(0, 1).\n",
    "\n",
    "    X2 ~ log(\\beta * (N(0, 1)) for n_features_2. Here \\beta is a vector that's linearly\n",
    "    decreasing as the dimensionality increases.\n",
    "    \"\"\"\n",
    "    # generate X1 with linear relationship to y\n",
    "    x_1, x1y = linear(n_samples, n_features, noise=noise_1)\n",
    "    x_1 = np.vstack((x_1, x1y))\n",
    "\n",
    "    # now add noise dimensions for x_1\n",
    "    x_1 = np.hstack((x_1, np.random.standard_normal((len(x_1), noise_dims))))\n",
    "\n",
    "    # generate independent X2 that has logarithmic separation\n",
    "    x_2, x2y = logarithmic(n_samples, n_features_2, noise=noise_2)\n",
    "    x_2 = np.vstack((x_2, x2y))\n",
    "\n",
    "    # now add noise dimensions for x_2\n",
    "    x_2 = np.hstack((x_2, np.random.standard_normal((len(x_2), noise_dims_2))))\n",
    "\n",
    "    # stack them together\n",
    "    x = np.hstack((x_1, x_2))\n",
    "\n",
    "    # now generate y, which is a function of both X1 and X2\n",
    "    y = np.array([0] * (n_samples) + [1] * (n_samples)).reshape(-1, 1).ravel()\n",
    "    # print(x_1.shape, x_2.shape, y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "43cfc1dd-5c0c-49cb-8541-8b933bacc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directindirect(\n",
    "    n_samples, n_features, n_features_2, noise_dims, class_probs, seed\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    fixed_center = rng.standard_normal(size=(n_features_2,))\n",
    "    centers = [fixed_center, fixed_center]\n",
    "\n",
    "    covariances = [\n",
    "        make_spd_matrix(n_dim=n_features_2, random_state=seed),\n",
    "        make_spd_matrix(n_dim=n_features_2, random_state=seed + 123),\n",
    "    ]\n",
    "\n",
    "    Xs, y = make_gaussian_mixture(\n",
    "        centers,\n",
    "        covariances,\n",
    "        n_samples=n_samples,\n",
    "        noise=1.0,\n",
    "        noise_dims=0,\n",
    "        shuffle=True,\n",
    "        class_probs=class_probs,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    Xs[0] = Xs[0][:, :n_features]\n",
    "    # print([x.shape for x in Xs])\n",
    "    # print(Xs[0].shape)\n",
    "    noise_arr = rng.standard_normal(size=(n_samples, noise_dims))\n",
    "    # print(noise_arr.shape)\n",
    "    # Xs[0] = np.hstack((Xs[0], ))\n",
    "    signal_X = np.hstack((Xs[0], noise_arr, Xs[1]))\n",
    "\n",
    "    return signal_X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54f54652-a093-45a8-ba98-b5b179073a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_directindirect(\n",
    "#     n_samples,\n",
    "#     n_features,\n",
    "#     n_features_2,\n",
    "#     noise_dims,\n",
    "#     noise_dims_2,\n",
    "#     seed,\n",
    "#     noise_1=False,\n",
    "#     noise_2=False,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     TODO\n",
    "#     \"\"\"\n",
    "#     # generate X1 with linear relationship to y\n",
    "#     x_1, x1y = linear(n_samples, n_features, noise=noise_1)\n",
    "\n",
    "#     # generate X2 as a function of X1\n",
    "#     x = np.random.normal(size=(n, p))\n",
    "#     eps = np.random.normal(size=(n_samples, n_features_2))\n",
    "#     if coeffs is None:\n",
    "#         coeffs = np.array([np.exp(-0.0022 * (i + 10)) for i in range(n_features_2)])\n",
    "#     x_21 = x1y * coeffs + noise_2 * eps\n",
    "\n",
    "#     x_2, x2y = logarithmic(n_samples, n_features_2, noise=noise_2)\n",
    "\n",
    "#     x_1 = np.vstack((x_1, x1y))\n",
    "#     # now add noise dimensions for x_1\n",
    "#     x_1 = np.hstack((x_1, np.random.standard_normal((len(x_1), noise_dims))))\n",
    "\n",
    "#     x_2 = np.vstack((x_2, x2y))\n",
    "#     # now add noise dimensions for x_2\n",
    "#     x_2 = np.hstack((x_2, np.random.standard_normal((len(x_2), noise_dims_2))))\n",
    "\n",
    "#     # stack them together\n",
    "#     x = np.hstack((x_1, x_2))\n",
    "\n",
    "#     # now generate y, which is a function of both X1 and X2\n",
    "#     y = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2)).reshape(-1, 1).ravel()\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4b53b5e9-5fd7-4a8f-9fa2-480adfc95dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confounder(\n",
    "    n_samples,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    seed,\n",
    "    noise_1=False,\n",
    "    noise_2=False,\n",
    "):\n",
    "    \"\"\"Make a X1, X2, Y with a confounded relationship between X2 and Y.\n",
    "\n",
    "    Here, X2 will be conditionally independent of Y given X1.\n",
    "    X1 ~ N(0, 1) for n_features. Then X1 is concatenated with noise dimensions N(0, 1).\n",
    "\n",
    "    X2 ~ \\beta * (N(0, 1)) for n_features_2. Here \\beta is a vector that's linearly\n",
    "    decreasing as the dimensionality increases.\n",
    "    \"\"\"\n",
    "    # generate X1 with linear relationship to y\n",
    "    x_1, x1y = linear(n_samples, n_features, noise=noise_1)\n",
    "    x_1 = np.vstack((x_1, x1y))\n",
    "    # now add noise dimensions for x_1\n",
    "    x_1 = np.hstack((x_1, np.random.standard_normal((len(x_1), noise_dims))))\n",
    "\n",
    "    # generate X2 as a function of X1\n",
    "    eps = np.random.standard_normal(size=(n_samples * 2, n_features_2))\n",
    "\n",
    "    # n_features x n_features\n",
    "    rand_U = ortho_group.rvs(n_features)[:, :n_features]\n",
    "    # n_features_2 x n_features_2\n",
    "    rand_V = ortho_group.rvs(n_features_2)[:, :n_features_2].T\n",
    "    # n_features x n_features_2\n",
    "    svals = np.zeros((n_features, n_features_2))\n",
    "    np.fill_diagonal(svals, [np.exp(-0.022 * i) for i in range(n_features)])\n",
    "\n",
    "    # print(rand_U.shape, svals.shape, rand_V.shape)\n",
    "    # n_features x n_features_2\n",
    "    coeff_arr = rand_U @ svals @ rand_V\n",
    "\n",
    "    # coeffs = np.array([np.exp(-0.022 * (i)) for i in range(n_features_2)])\n",
    "    x_2 = x_1[:, :n_features] @ coeff_arr + noise_2 * eps\n",
    "\n",
    "    # x_2 = np.vstack((x_2, x2y))\n",
    "    # now add noise dimensions for x_2\n",
    "    x_2 = np.hstack((x_2, np.random.standard_normal((len(x_2), noise_dims_2))))\n",
    "\n",
    "    # stack them together\n",
    "    x = np.hstack((x_1, x_2))\n",
    "\n",
    "    # now generate y, which is a function of both X1 and X2\n",
    "    y = np.array([0] * (n_samples) + [1] * (n_samples)).reshape(-1, 1).ravel()\n",
    "    # print(x.shape, y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e5a69-4f06-4da0-b430-6bb1d571e91a",
   "metadata": {},
   "source": [
    "# Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a04351a5-1383-48ea-8834-7838ef7e507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features in the first view\n",
    "n_features = 10\n",
    "noise_dims = 90\n",
    "n_features_2 = 32\n",
    "noise_dims_2 = 4096 - n_features_2 - (n_features + noise_dims)\n",
    "n_features_2_full = 4096\n",
    "\n",
    "n_samples_full = 2048 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2fff3659-1931-40d7-b93b-791c406a113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6a815f65-b362-4966-9c04-417892fd6c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 90 32 3964\n"
     ]
    }
   ],
   "source": [
    "print(n_features, noise_dims, n_features_2, noise_dims_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc83af-c6c8-4968-904e-59a610a6395e",
   "metadata": {},
   "source": [
    "## Log (Collider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8a446503-8108-429c-a110-97e72c76c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_collider_dataset(\n",
    "    n_samples,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    n_repeats,\n",
    "):\n",
    "    for idx in range(n_repeats):\n",
    "        rng = np.random.default_rng(seed * idx)\n",
    "        X, y = make_log_collider(\n",
    "            n_samples,\n",
    "            n_features,\n",
    "            n_features_2,\n",
    "            noise_dims,\n",
    "            noise_dims_2,\n",
    "            seed * idx,\n",
    "            noise_1=True,\n",
    "            noise_2=True,\n",
    "        )\n",
    "\n",
    "        n_features_ends = [n_features + noise_dims, X.shape[1]]\n",
    "        # print(X.shape, y.shape, n_features_ends)\n",
    "        np.savez(\n",
    "            f\"/Users/adam2392/Desktop/cancer/data/log_collider/log_collider_{idx}.npz\",\n",
    "            X=X,\n",
    "            y=y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9144c907-e42b-47b7-b44a-79fe3e3f759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(n_samples_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "724f1d1e-da48-4ccc-8dbf-0cc1280e1151",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_log_collider_dataset(\n",
    "    n_samples_full,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    n_repeats,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ec953-a6fe-4e40-b15c-62410ac47689",
   "metadata": {},
   "source": [
    "## Confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cd83166a-a1e3-41dc-be1e-f652661b6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confounder_dataset(\n",
    "    n_samples,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    n_repeats,\n",
    "):\n",
    "    for idx in range(n_repeats):\n",
    "        rng = np.random.default_rng(seed * idx)\n",
    "        X, y = make_confounder(\n",
    "            n_samples,\n",
    "            n_features,\n",
    "            n_features_2,\n",
    "            noise_dims,\n",
    "            noise_dims_2,\n",
    "            seed * idx,\n",
    "            noise_1=True,\n",
    "            noise_2=True,\n",
    "        )\n",
    "\n",
    "        make_log_collider(\n",
    "            n_samples,\n",
    "            n_features,\n",
    "            n_features_2,\n",
    "            noise_dims,\n",
    "            noise_dims_2,\n",
    "            seed * idx,\n",
    "            noise_1=True,\n",
    "            noise_2=True,\n",
    "        )\n",
    "\n",
    "        n_features_ends = [n_features + noise_dims, X.shape[1]]\n",
    "        # print(X.shape, y.shape, n_features_ends)\n",
    "        np.savez(\n",
    "            f\"/Users/adam2392/Desktop/cancer/data/confounder/confounder_{idx}.npz\",\n",
    "            X=X,\n",
    "            y=y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "68eb1e1f-82fd-42d7-96ba-e10fe1016542",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confounder_dataset(\n",
    "    n_samples_full,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    n_repeats,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c35e1-a667-45e1-add3-1525f26d04c5",
   "metadata": {},
   "source": [
    "## Direct/Indirect Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0ed8ca87-5739-40ed-a2a1-8e5c6b97d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directindirect_dataset(\n",
    "    n_samples,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    n_repeats,\n",
    "    seed,\n",
    "):\n",
    "    for idx in range(n_repeats):\n",
    "        rng = np.random.default_rng(seed * idx)\n",
    "        signal_X, y = make_directindirect(\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,\n",
    "            n_features_2=n_features_2,\n",
    "            noise_dims=100 - n_features,\n",
    "            class_probs=class_probs,\n",
    "            seed=seed * idx,\n",
    "        )\n",
    "        X = np.hstack(\n",
    "            (\n",
    "                signal_X,\n",
    "                rng.standard_normal(size=(n_samples, noise_dims_2)),\n",
    "            )\n",
    "        )\n",
    "        # # signal_X = np.hstack((signal_X, Xs[1][:, :4]))\n",
    "        # n_features_ends_one = 100\n",
    "        # signal_X = np.hstack((signal_X, Xs[1]))\n",
    "        # second_view_dim = Xs[1].shape[1]\n",
    "\n",
    "        n_features_ends = [n_features + noise_dims, None]\n",
    "        # _X = signal_X.copy()\n",
    "        # second_view_dim = n_features_2 - n_features_ends[0]\n",
    "        # # # if n_features_2_ - second_view_dim > 0:\n",
    "        # _X = np.hstack(\n",
    "        #     (\n",
    "        #         _X,\n",
    "        #         rng.standard_normal(\n",
    "        #             size=(n_samples, n_features_2_list[-1] - second_view_dim)\n",
    "        #         ),\n",
    "        #     )\n",
    "        # )\n",
    "        # X = _X.copy()\n",
    "        n_features_ends[1] = X.shape[1]\n",
    "        print(signal_X.shape, X.shape, n_features_ends)\n",
    "        np.savez(\n",
    "            f\"/Users/adam2392/Desktop/cancer/data/direct-indirect/direct-indirect_{idx}.npz\",\n",
    "            X=X,\n",
    "            y=y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "11f0d592-24ad-4694-8b9f-2ae110e6a221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_directindirect_dataset(\n",
    "    n_samples_full,\n",
    "    n_features,\n",
    "    n_features_2,\n",
    "    noise_dims,\n",
    "    noise_dims_2,\n",
    "    n_repeats,\n",
    "    seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad0544-0229-437f-b792-e5bd1b62b2d3",
   "metadata": {},
   "source": [
    "## Independent View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "02dcb6a3-0aec-4cd7-9970-10402bc7cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_independent_dataset(\n",
    "    n_samples, n_features, n_features_2, noise_dims, n_repeats, seed\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for idx in range(n_repeats):\n",
    "        n_features_begin = 0\n",
    "        signal_X, y = make_classification(\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features + noise_dims,\n",
    "            n_redundant=0,\n",
    "            shuffle=True,\n",
    "            n_informative=n_features,\n",
    "            random_state=seed * idx,\n",
    "        )\n",
    "        n_features_ends = [n_features + noise_dims, None]\n",
    "\n",
    "        _X = np.hstack(\n",
    "            (signal_X, rng.standard_normal(size=(n_samples, n_features_2_list[-1])))\n",
    "        )\n",
    "        X = _X.copy()\n",
    "        n_features_ends[1] = X.shape[1]\n",
    "\n",
    "        # print(X.shape, signal_X.shape, n_features_ends)\n",
    "        np.savez(\n",
    "            f\"/Users/adam2392/Desktop/cancer/data/independent/independent_{idx}.npz\",\n",
    "            X=X,\n",
    "            y=y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b9d79e07-9389-4d7e-8b3a-a674bf285779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096 90 10\n"
     ]
    }
   ],
   "source": [
    "print(n_features_2, noise_dims, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f1b6117e-5b1f-4557-9cde-a215864f2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_independent_dataset(\n",
    "    n_samples_full, n_features, n_features_2, noise_dims, n_repeats, seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab53a32-6dfe-430c-87bd-f514915d1d70",
   "metadata": {},
   "source": [
    "# Run Co-MIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fa61da8a-fef7-4ec8-8306-32a0c237861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_parallel_comight(\n",
    "    idx, n_samples, seed, n_features_2, test_size, sim_type, rootdir, output_folder\n",
    "):\n",
    "    \"\"\"Run parallel job on pre-generated data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx : int\n",
    "        The index of the pre-generated dataset, stored as npz file.\n",
    "    n_samples : int\n",
    "        The number of samples to keep.\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    n_features_2 : int\n",
    "        The number of dimensions to keep in feature set 2.\n",
    "    test_size : float\n",
    "        The size of the test set to use for predictive-model based tests.\n",
    "    sim_type : str\n",
    "        The simulation type. Either 'independent', 'collider', 'confounder',\n",
    "        or 'direct-indirect'.\n",
    "    rootdir : str\n",
    "        The root directory where 'data/' and 'output/' will be.\n",
    "    run_cdcorr : bool, optional\n",
    "        Whether or not to run conditional dcorr, by default True.\n",
    "    \"\"\"\n",
    "    n_jobs = 1\n",
    "    n_features_ends = [100, None]\n",
    "\n",
    "    # set output directory to save npz files\n",
    "    output_dir = os.path.join(rootdir, f\"output/{output_folder}/{sim_type}/\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # load data\n",
    "    npy_data = np.load(os.path.join(rootdir, f\"data/{sim_type}/{sim_type}_{idx}.npz\"))\n",
    "\n",
    "    X = npy_data[\"X\"]\n",
    "    y = npy_data[\"y\"]\n",
    "\n",
    "    X = X[:, : 100 + n_features_2]\n",
    "    if n_samples < X.shape[0]:\n",
    "        cv = StratifiedShuffleSplit(n_splits=1, train_size=n_samples)\n",
    "        for train_idx, _ in cv.split(X, y):\n",
    "            continue\n",
    "        X = X[train_idx, :]\n",
    "        y = y[train_idx, ...].squeeze()\n",
    "    assert len(X) == len(y)\n",
    "    assert len(y) == n_samples\n",
    "    n_features_ends[1] = X.shape[1]\n",
    "\n",
    "    est = FeatureImportanceForestClassifier(\n",
    "        estimator=HonestForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            tree_estimator=MultiViewDecisionTreeClassifier(\n",
    "                max_features=[max_features, min(n_features_2, max_features * 100)],\n",
    "                feature_set_ends=n_features_ends,\n",
    "                apply_max_features_per_feature_set=True,\n",
    "            ),\n",
    "            random_state=seed,\n",
    "            honest_fraction=0.5,\n",
    "            n_jobs=n_jobs,\n",
    "        ),\n",
    "        random_state=seed,\n",
    "        test_size=test_size,\n",
    "        sample_dataset_per_tree=False,\n",
    "    )\n",
    "\n",
    "    # now compute the pvalue when shuffling X2\n",
    "    covariate_index = np.arange(n_features_ends[0], n_features_ends[1])\n",
    "\n",
    "    # Estimate CMI with\n",
    "    mi_rf, pvalue = est.test(\n",
    "        X,\n",
    "        y,\n",
    "        covariate_index=covariate_index,\n",
    "        return_posteriors=True,\n",
    "        metric=\"mi\",\n",
    "    )\n",
    "    comight_posteriors_x2 = est.observe_posteriors_\n",
    "    comight_null_posteriors_x2 = est.permute_posteriors_\n",
    "\n",
    "    samples = est.observe_samples_\n",
    "    permute_samples = est.permute_samples_\n",
    "\n",
    "    assert np.isnan(comight_posteriors_x2[:, samples, :]).sum() == 0\n",
    "\n",
    "    np.savez(\n",
    "        os.path.join(output_dir, f\"comight_{n_samples}_{n_features_2}_{idx}.npz\"),\n",
    "        n_samples=n_samples,\n",
    "        n_features_2=n_features_2,\n",
    "        y_true=y,\n",
    "        comight_pvalue=pvalue,\n",
    "        comight_mi=mi_rf,\n",
    "        comight_posteriors_x2=comight_posteriors_x2,\n",
    "        comight_null_posteriors_x2=comight_null_posteriors_x2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "892b6fab-01f7-4d74-a472-200021b59236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
      "[64, 128, 256, 512, 1024]\n"
     ]
    }
   ],
   "source": [
    "# hard-coded parameters\n",
    "n_estimators = 500\n",
    "max_features = 0.3\n",
    "test_size = 0.2\n",
    "n_jobs = -1\n",
    "\n",
    "n_samples = 512\n",
    "n_features_2 = 4096\n",
    "\n",
    "max_fpr = 0.1\n",
    "\n",
    "# number of features in the second view\n",
    "pows = np.arange(2, 13, dtype=int)\n",
    "n_features_2_list = [2**pow for pow in pows]\n",
    "print(n_features_2_list)\n",
    "\n",
    "# n_samples_list = [2**x for x in range(6, 12)]\n",
    "n_samples_list = [2**x for x in range(6, 11)]\n",
    "print(n_samples_list)\n",
    "class_probs = [0.5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b0c13-3474-43b9-bd34-ef2b6f6b7ebc",
   "metadata": {},
   "source": [
    "## Collider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d252c4bb-d5b2-4a72-87cc-ff83cb061992",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"/Users/adam2392/Desktop/cancer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fda9c852-4ed2-4716-b55d-427b00922bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 4096\n"
     ]
    }
   ],
   "source": [
    "print(n_samples, n_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1569733c-a1ec-43d2-b196-f177ab366a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples,\n",
    "        seed + 1,\n",
    "        n_features_2_,\n",
    "        test_size,\n",
    "        \"log_collider\",\n",
    "        rootdir,\n",
    "        \"varying-dimensionality\",\n",
    "    )\n",
    "    for (idx_, n_features_2_) in product(range(n_repeats), n_features_2_list)\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "62db2710-fade-4371-b341-f8b4b4ed5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples_,\n",
    "        seed + 1,\n",
    "        n_features_2,\n",
    "        test_size,\n",
    "        \"log_collider\",\n",
    "        rootdir,\n",
    "        \"varying-samples\",\n",
    "    )\n",
    "    for (idx_, n_samples_) in product(range(n_repeats), n_samples_list)\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "73cdc54b-5d1d-4534-aa1c-b99286903508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52622b-e913-424d-9d5c-354a89674367",
   "metadata": {},
   "source": [
    "## Confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e6022345-5dc0-4de0-9598-52e854aecd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"/Users/adam2392/Desktop/cancer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "75f49ce9-5cd5-4a14-875a-c3892c9e3b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9a9e2f4a-5344-4c74-b65e-9df7545e7630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples,\n",
    "        seed + 1,\n",
    "        n_features_2_,\n",
    "        test_size,\n",
    "        \"confounder\",\n",
    "        rootdir,\n",
    "        \"varying-dimensionality\",\n",
    "    )\n",
    "    for (idx_, n_features_2_) in product(range(n_repeats), n_features_2_list)\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cfb6af17-d1de-4587-911f-d7a617e60285",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_2 = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d2b3098e-78fa-42a2-b622-a2b4166896ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples_,\n",
    "        seed + 1,\n",
    "        n_features_2,\n",
    "        test_size,\n",
    "        \"confounder\",\n",
    "        rootdir,\n",
    "        \"varying-samples\",\n",
    "    )\n",
    "    for (idx_, n_samples_) in product(range(n_repeats), n_samples_list)\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "471b7aa7-de56-488a-9eca-0a65625c2ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b0ccc-fc1f-4551-8c0f-5b83997ffb77",
   "metadata": {},
   "source": [
    "## Direct-Indirect (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86c1c6-da5e-4331-a66e-4834d8dba74a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples,\n",
    "        seed + 1,\n",
    "        n_features_2_,\n",
    "        test_size,\n",
    "        \"direct-indirect\",\n",
    "        rootdir,\n",
    "        \"varying-dimensionality\",\n",
    "    )\n",
    "    for (idx_, n_features_2_) in product(range(n_repeats), n_features_2_list)\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626ab06-7921-405b-af52-e9e595656ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_2 = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c1bee-f88d-49f0-a6d7-c1365fd9e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples_,\n",
    "        seed + 1,\n",
    "        n_features_2,\n",
    "        test_size,\n",
    "        \"direct-indirect\",\n",
    "        rootdir,\n",
    "        \"varying-samples\",\n",
    "    )\n",
    "    for (idx_, n_samples_) in product(range(n_repeats), n_samples_list)\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f54f8-3eb9-4032-b3fa-89b7200996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09fe2b-dfe5-47e4-8a81-97a60b1d13f9",
   "metadata": {},
   "source": [
    "## Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e08c5-0743-4fcb-93c4-a3159db04cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples,\n",
    "        seed + 1,\n",
    "        n_features_2_,\n",
    "        test_size,\n",
    "        \"independent\",\n",
    "        rootdir,\n",
    "        \"varying-dimensionality\",\n",
    "    )\n",
    "    for (idx_, n_features_2_) in product(range(n_repeats), n_features_2_list)\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328cc198-2505-44cb-b29f-0f3c616cc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_2 = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c781df-31b3-4bb5-bb0c-6bf45887da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=-1)(\n",
    "    delayed(_run_parallel_comight)(\n",
    "        idx_,\n",
    "        n_samples_,\n",
    "        seed + 1,\n",
    "        n_features_2,\n",
    "        test_size,\n",
    "        \"independent\",\n",
    "        rootdir,\n",
    "        \"varying-samples\",\n",
    "    )\n",
    "    for (idx_, n_samples_) in product(range(n_repeats), n_samples_list)\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee3430-0811-44a7-b28a-818be9e03144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7f59c-501c-4bfc-bdae-c62abcdbeb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sktree",
   "language": "python",
   "name": "sktree"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
