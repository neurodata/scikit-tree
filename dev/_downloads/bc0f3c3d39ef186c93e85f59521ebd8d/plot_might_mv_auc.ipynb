{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compute partial AUC using multi-view MIGHT (MV-MIGHT)\n\nAn example using :class:`~sktree.stats.FeatureImportanceForestClassifier` for nonparametric\nmultivariate hypothesis test, on simulated mutli-view datasets. Here, we present\nhow to estimate partial AUROC from a multi-view feature set.\n\nWe simulate a dataset with 510 features, 1000 samples, and a binary class target variable.\nThe first 10 features (X) are strongly correlated with the target, and the second\nfeature set (W) is weakly correlated with the target (y).\n\nWe then use MV-MIGHT to calculate the partial AUC of these sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom scipy.special import expit\n\nfrom sktree import HonestForestClassifier\nfrom sktree.stats import FeatureImportanceForestClassifier\nfrom sktree.tree import DecisionTreeClassifier, MultiViewDecisionTreeClassifier\n\nseed = 12345\nrng = np.random.default_rng(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate data\nWe simulate the two feature sets, and the target variable. We then combine them\ninto a single dataset to perform hypothesis testing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 1000\nn_features_set = 500\nmean = 1.0\nsigma = 2.0\nbeta = 5.0\n\nunimportant_mean = 0.0\nunimportant_sigma = 4.5\n\n# first sample the informative features, and then the uniformative features\nX_important = rng.normal(loc=mean, scale=sigma, size=(n_samples, 10))\nX_unimportant = rng.normal(\n    loc=unimportant_mean, scale=unimportant_sigma, size=(n_samples, n_features_set)\n)\nX = np.hstack([X_important, X_unimportant])\n\n# simulate the binary target variable\ny = rng.binomial(n=1, p=expit(beta * X_important[:, :10].sum(axis=1)), size=n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use partial AUC as test statistic\nYou can specify the maximum specificity by modifying ``max_fpr`` in ``statistic``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_estimators = 125\nmax_features = 100\nmetric = \"auc\"\ntest_size = 0.2\nn_jobs = -1\nhonest_fraction = 0.5\nmax_fpr = 0.1\n\nest_mv = FeatureImportanceForestClassifier(\n    estimator=HonestForestClassifier(\n        n_estimators=n_estimators,\n        max_features=max_features,\n        tree_estimator=MultiViewDecisionTreeClassifier(feature_set_ends=[10, 10 + n_features_set]),\n        honest_fraction=honest_fraction,\n        n_jobs=n_jobs,\n    ),\n    random_state=seed,\n    test_size=test_size,\n    permute_per_tree=True,\n    sample_dataset_per_tree=True,\n)\n\n# we test with the multi-view setting, thus should return a higher AUC\nstat, posterior_arr, samples = est_mv.statistic(\n    X,\n    y,\n    metric=metric,\n    return_posteriors=True,\n    max_fpr=max_fpr,\n)\n\nprint(f\"ASH-90 / Partial AUC: {stat}\")\nprint(f\"Shape of Observed Samples: {samples.shape}\")\nprint(f\"Shape of Tree Posteriors for the positive class: {posterior_arr.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repeat without multi-view\nThis feature set has a smaller statistic, which is expected due to its lack of multi-view setting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "est = FeatureImportanceForestClassifier(\n    estimator=HonestForestClassifier(\n        n_estimators=n_estimators,\n        max_features=max_features,\n        tree_estimator=DecisionTreeClassifier(),\n        honest_fraction=honest_fraction,\n        n_jobs=n_jobs,\n    ),\n    random_state=seed,\n    test_size=test_size,\n    permute_per_tree=True,\n    sample_dataset_per_tree=True,\n)\n\nstat, posterior_arr, samples = est.statistic(\n    X,\n    y,\n    metric=metric,\n    return_posteriors=True,\n    max_fpr=max_fpr,\n)\n\nprint(f\"ASH-90 / Partial AUC: {stat}\")\nprint(f\"Shape of Observed Samples: {samples.shape}\")\nprint(f\"Shape of Tree Posteriors for the positive class: {posterior_arr.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All posteriors are saved within the model\nExtract the results from the model variables anytime. You can save the model with ``pickle``.\n\nASH-90 / Partial AUC: ``est_mv.observe_stat_``\n\nObserved Samples: ``est_mv.observe_samples_``\n\nTree Posteriors for the positive class: ``est_mv.observe_posteriors_``\n(n_trees, n_samples_test, 1)\n\nTrue Labels: ``est_mv.y_true_final_``\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}