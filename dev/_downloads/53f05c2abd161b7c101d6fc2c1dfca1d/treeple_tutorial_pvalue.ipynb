{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Treeple tutorial for calculating p-value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import entropy\n\nfrom sktree.datasets import make_trunk_classification\nfrom sktree.ensemble import HonestForestClassifier\nfrom sktree.stats import build_hyppo_oob_forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Independence Testing\n\nGiven samples from ``X`` and ``Y``, the independent hypothesis and its\nalternative are stated as:\n\n\\begin{align}H_0 : F_{XY} = F_X F_Y\\end{align}\n\n\\begin{align}H_A : F_{XY} \\neq F_X F_Y\\end{align}\n\nP-value is the probability of observing a statistic more extreme than the null.\nBy computing the p-value using ``treeple``, we can test if $H_0$\nwould be rejected, which confirms that X and Y are not independent. The p-value is\ngenerated by comparing the observed statistic difference with permuted\ndifferences, using mutual information as an example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MI\n\nMutual Information (*MI*) measures the mutual dependence between *X* and\n*Y*. It can be calculated by the difference between the class entropy\n(``H(Y)``) and the conditional entropy (``H(Y | X)``):\n\n\\begin{align}I(X; Y) = H(Y) - H(Y\\mid X)\\end{align}\n\nWith a binary class simulation as an example, this tutorial will show\nhow to use ``treeple`` to use the statistic and the p-value.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a simulation with two gaussians\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a binary class simulation with two gaussians\n# 500 samples for each class, class zero is standard\n# gaussian, and class one has a mean at one\nX, y = make_trunk_classification(\n    n_samples=1000,\n    n_dim=1,\n    mu_0=0,\n    mu_1=1,\n    n_informative=1,\n    seed=1,\n)\n\n\n# scatter plot the samples\nplt.hist(X[:500], bins=15, alpha=0.6, color=\"blue\", label=\"negative\")\nplt.hist(X[500:], bins=15, alpha=0.6, color=\"red\", label=\"positive\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate observed posteriors\n\nThe observed posteriors represent the original distribution of ``X`` and ``Y``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initialize the forest with 100 trees\nest = HonestForestClassifier(\n    n_estimators=100,\n    max_samples=1.6,\n    max_features=0.3,\n    bootstrap=True,\n    stratify=True,\n    random_state=1,\n)\n\n# fit the model and obtain the tree posteriors\n_, observe_proba = build_hyppo_oob_forest(est, X, y)\n\n# generate forest posteriors for the two classes\nobserve_proba = np.nanmean(observe_proba, axis=0)\n\n\n# scatter plot the posterior probabilities for class one\nplt.hist(observe_proba[:500][:, 1], bins=30, alpha=0.6, color=\"blue\", label=\"negative\")\nplt.hist(observe_proba[500:][:, 1], bins=30, alpha=0.6, color=\"red\", label=\"positive\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate null posteriors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# shuffle the labels\nX_null = np.copy(X)\ny_null = np.copy(y)\nnp.random.shuffle(y_null)\n\n# initialize another forest with 100 trees\nest_null = HonestForestClassifier(\n    n_estimators=100,\n    max_samples=1.6,\n    max_features=0.3,\n    bootstrap=True,\n    stratify=True,\n    random_state=1,\n)\n\n# fit the model and obtain the tree posteriors\n_, null_proba = build_hyppo_oob_forest(est, X_null, y_null)\n\n# generate forest posteriors for the two classes\nnull_proba = np.nanmean(null_proba, axis=0)\n\n\n# scatter plot the posterior probabilities for class one\nplt.hist(null_proba[:500][:, 1], bins=30, alpha=0.6, color=\"blue\", label=\"negative\")\nplt.hist(null_proba[500:][:, 1], bins=30, alpha=0.6, color=\"red\", label=\"positive\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find the observed statistic difference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def Calculate_MI(y_true, y_pred_proba):\n    # calculate the conditional entropy\n    H_YX = np.mean(entropy(y_pred_proba, base=np.exp(1), axis=1))\n\n    # empirical count of each class (n_classes)\n    _, counts = np.unique(y_true, return_counts=True)\n    # calculate the entropy of labels\n    H_Y = entropy(counts, base=np.exp(1))\n    return H_Y - H_YX\n\n\nmi = Calculate_MI(y, observe_proba)\nmi_null = Calculate_MI(y, null_proba)\n\nobserved_diff = mi - mi_null\nprint(\"Observed statistic difference =\", round(observed_diff, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Permute the trees\n\nIn this case, permuting the tree posteriors is equivalent to permuting\nthe trees in the two forests.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PERMUTE = 10000\nmix_diff = []\n\n# Collect all the tree posteriors\nproba = np.vstack((observe_proba, null_proba))\nfor i in range(PERMUTE):\n\n    # permute the posteriors\n    np.random.shuffle(proba)\n\n    # calculate the statistic for\n    # the two mixed forest posteriors\n    mi_mix_one = Calculate_MI(y, proba[:100])\n    mi_mix_two = Calculate_MI(y, proba[100:])\n    mix_diff.append(mi_mix_one - mi_mix_two)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate the p-value\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pvalue = (1 + (mix_diff >= observed_diff).sum()) / (1 + PERMUTE)\n\nprint(\"p-value is:\", round(pvalue, 2))\nif pvalue < 0.05:\n    print(\"The null hypothesis is rejected.\")\nelse:\n    print(\"The null hypothesis is not rejected.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}