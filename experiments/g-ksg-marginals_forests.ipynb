{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.special\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "from numpy.typing import ArrayLike\n",
    "from collections import defaultdict\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sktree.ensemble import UnsupervisedObliqueRandomForest, UnsupervisedRandomForest\n",
    "from sktree.tree import compute_forest_similarity_matrix\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-KSG\n",
    "Use Unsupervised Random Forests to estimate MI\n",
    "### Steps\n",
    "1. Train a forest with joint dataset (Z = (X, Y)) get d_xy \n",
    "        - get d_z: k-th closest d_xy\n",
    "2. Train 2 forests with marginals dataset (X, Y)\n",
    "3. get geodesic distance d_x, and d_y from 2 \n",
    "        - get \n",
    "4. calculate MI from d_x, d_y, d_xy as  \n",
    "\n",
    "$\\Psi(k) - E[(\\Psi(n_x) + \\Psi(n_y))] + \\Psi(n)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI =  4.465020319672863\n",
      "MI = hk - E[hx + hy] + hn = -0.5772156649015329 - (0.6113557636698957 + 0.8967741310168345) + 6.550365879261126\n",
      "4.465020319672863\n",
      "true_mi =  4.610170185988091\n"
     ]
    }
   ],
   "source": [
    "n = 700\n",
    "px = py = 1\n",
    "k = 1\n",
    "alpha = 0.01\n",
    "n_estimators = 100\n",
    "max_features = 0.3\n",
    "max_depth = None\n",
    "min_samples_leaf = 2\n",
    "X = np.random.uniform(0, 1, size=(n,px))\n",
    "N = np.random.uniform(-alpha/2, alpha/2, size=(n,px))\n",
    "Y = X + N\n",
    "Z = np.hstack([X, Y])\n",
    "n = X.shape[0]\n",
    "px = X.shape[1]\n",
    "py = Y.shape[1]\n",
    "assert X.shape[0] == Y.shape[0]\n",
    "null_x = np.zeros(shape=(n, py))\n",
    "null_y = np.zeros(shape=(n, px))\n",
    "X0 = np.hstack([X, null_x])\n",
    "Y0 = np.hstack([Y, null_y])\n",
    "estimator_X = UnsupervisedRandomForest(n_estimators=n_estimators, max_features=max_features, min_samples_leaf=min_samples_leaf)\n",
    "estimator_Y = UnsupervisedRandomForest(n_estimators=n_estimators, max_features=max_features, min_samples_leaf=min_samples_leaf)\n",
    "estimator_X.fit(X0)\n",
    "estimator_Y.fit(Y0)\n",
    "d_x = 1-estimator_X.compute_similarity_matrix(Z)\n",
    "d_y = 1-estimator_Y.compute_similarity_matrix(Z)\n",
    "d_z = np.maximum(d_x, d_y)\n",
    "# if d_z = 0, then get L2 norm\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if (d_z[i, j]==0.0) and (i != j):\n",
    "            d_z[i, j] = np.linalg.norm(Z[i, :] - Z[j, :])/n_estimators\n",
    "n_x = []\n",
    "n_y = []\n",
    "for sample_id in range(n):\n",
    "    d_zik = np.sort(d_z[sample_id])[k]\n",
    "    n_xi = np.sum(np.array(d_x[sample_id] < d_zik)) -1 # for itself\n",
    "    n_x.append(n_xi)\n",
    "    n_yi = np.sum(np.array(d_y[sample_id] < d_zik)) -1 # for itself\n",
    "    n_y.append(n_yi)\n",
    "\n",
    "mi = ksg_mi(k,n,np.array(n_x), np.array(n_y), verbose=True)\n",
    "print(mi)\n",
    "true_mi = alpha/2 - np.log(alpha)\n",
    "print(\"true_mi = \", true_mi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class G_KSG_MI_Estimator:\n",
    "    def __init__(self, k: int = 1, n_estimators: int = 100, max_features: float = 1.0, max_depth: Optional[int] = None, min_samples_leaf: int = 2):\n",
    "        self.k = k\n",
    "        self.n = None\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_x = None\n",
    "        self.n_y = None\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.p = None\n",
    "        self.estimator_X = UnsupervisedRandomForest(n_estimators=self.n_estimators, max_features=self.max_features, min_samples_leaf=self.min_samples_leaf)\n",
    "        self.estimator_Y = UnsupervisedRandomForest(n_estimators=self.n_estimators, max_features=self.max_features, min_samples_leaf=self.min_samples_leaf)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # joint space Z = [X, Y]\n",
    "        Z = np.hstack([X, Y])\n",
    "        self.n = X.shape[0]\n",
    "        self.px = X.shape[1]\n",
    "        self.py = Y.shape[1]\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        # Z = np.hstack([X, Y])\n",
    "        null_x = np.zeros(shape=(self.n, self.py))\n",
    "        null_y = np.zeros(shape=(self.n, self.px))\n",
    "        X0 = np.hstack([X, null_x])\n",
    "        Y0 = np.hstack([Y, null_y])\n",
    "        self.estimator_X.fit(X0)\n",
    "        self.estimator_Y.fit(Y0)\n",
    "        self.d_x = 1-self.estimator_X.compute_similarity_matrix(Z)\n",
    "        self.d_y = 1-self.estimator_Y.compute_similarity_matrix(Z)\n",
    "        self.d_z = np.maximum(self.d_x, self.d_y)\n",
    "        # if d_z = 0, then get L2 norm\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                if (self.d_z[i, j]==0.0) and (i != j):\n",
    "                    self.d_z[i, j] = np.linalg.norm(Z[i, :] - Z[j, :])/self.n_estimators\n",
    "\n",
    "        self.n_x = []\n",
    "        self.n_y = []\n",
    "        for sample_id in range(n):\n",
    "            d_zik = np.sort(self.d_z[sample_id])[k]\n",
    "            n_xi = np.sum(np.array(self.d_x[sample_id] < d_zik)) -1 # remove itself\n",
    "            self.n_x.append(n_xi)\n",
    "            n_yi = np.sum(np.array(self.d_y[sample_id] < d_zik)) -1 # remove itself\n",
    "            self.n_y.append(n_yi)\n",
    "        return self\n",
    "    \n",
    "    def ksg_mi(self, X, Y, verbose: bool = False):\n",
    "        self.fit(X, Y)\n",
    "        k = self.k\n",
    "        n = self.n\n",
    "        n_x = np.array(self.n_x)\n",
    "        n_y = np.array(self.n_y)\n",
    "        # compute the MI using the KSG estimator\n",
    "        # \\\\psi(k) - E[(\\\\psi(n_x) + \\\\psi(n_y))] + \\\\psi(n)\n",
    "        hk = scipy.special.digamma(k)\n",
    "        hx = scipy.special.digamma(n_x +1)\n",
    "        hy = scipy.special.digamma(n_y +1)\n",
    "        hn = scipy.special.digamma(n)\n",
    "        mi = hk - (hx + hy).mean() + hn\n",
    "        if verbose:\n",
    "            print(\"MI = \", mi)\n",
    "            print(f\"MI = hk - E[hx + hy] + hn = {hk} - ({hx.mean()} + {hy.mean()}) + {hn}\")\n",
    "        return mi\n",
    "\n",
    "n = 2000\n",
    "px = py = 1\n",
    "k = 1\n",
    "alpha = 0.01\n",
    "n_estimators = 100\n",
    "max_features = 0.3\n",
    "max_depth = None\n",
    "min_samples_leaf = 2\n",
    "X = np.random.uniform(0, 1, size=(n,px))\n",
    "N = np.random.uniform(-alpha/2, alpha/2, size=(n,px))\n",
    "Y = X + N\n",
    "\n",
    "estimator = G_KSG_MI_Estimator(k=k, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "mi = estimator.ksg_mi(X, Y, verbose=False)\n",
    "print(\"mi = \",mi)\n",
    "true_mi = alpha/2 - np.log(alpha)\n",
    "print(\"true_mi = \", true_mi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiemnt 0\n",
    "\n",
    "$X = Unif(0, 1)$  \n",
    "$Y = X + Unif(-\\alpha/2, \\alpha/2)$  \n",
    "$I(X; Y) = h(Y)-h(Z)= \\alpha/2 - \\log(\\alpha)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.7252256440870397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = [100]\n",
    "p = 1\n",
    "k = 1\n",
    "alpha = 0.01\n",
    "n_estimators = 100\n",
    "max_features = 0.3\n",
    "max_depth = None\n",
    "min_samples_leaf = 1\n",
    "num_trials = 1\n",
    "results = []\n",
    "times = []\n",
    "for n in tqdm(num_samples):\n",
    "    t0 = time.time()\n",
    "    _res = []\n",
    "    for _ in tqdm(range(num_trials)):\n",
    "        X = np.random.uniform(0, 1, size=(n,p))\n",
    "        N = np.random.uniform(-alpha/2, alpha/2, size=n)\n",
    "        Y = X + N\n",
    "        est = G_KSG_MI_Estimator(k=k, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        _res.append(est.ksg_mi(X, Y))\n",
    "    times.append(time.time() - t0)\n",
    "    results.append(np.mean(_res))\n",
    "\n",
    "# calulate the true MI \n",
    "true_mi = alpha/2 - np.log(alpha)\n",
    "print(results)\n",
    "plt.plot(num_samples, results, label=\"Estimated MI\", c=\"red\")\n",
    "plt.hlines(true_mi, xmin=0, xmax=num_samples[-1], label=\"True MI\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mi = alpha/2 - np.log(alpha)\n",
    "plt.plot(num_samples, results, label=\"Estimated MI\", c=\"red\")\n",
    "plt.hlines(true_mi, xmin=0, xmax=1000, label=\"True MI\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(range(1,5))\n",
    "n = list(range(1000))\n",
    "for k in tqdm(k):\n",
    "    hk = scipy.special.digamma(k)\n",
    "    hn = scipy.special.digamma(n)\n",
    "    plt.plot(n, hk + hn, label=f\"k={k}\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sktree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
